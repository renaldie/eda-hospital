{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f9b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result of 1 + 1 is **2**. This is a fundamental arithmetic operation where two numbers are combined by addition. In this case, adding 1 and 1 yields 2. No special context or complexity is required for this question. \\n\\n**Answer:**  \\n$$\\n\\\\boxed{2}\\n$$'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "\n",
    "from pydantic import SecretStr, Field\n",
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "load_dotenv(override=True)\n",
    "usage_callback = UsageMetadataCallbackHandler()\n",
    "\n",
    "\n",
    "class AppSettings(BaseSettings):\n",
    "    OPENAI_API_KEY: SecretStr = Field(\n",
    "        default_factory=lambda: SecretStr(os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "\n",
    "SETTINGS = AppSettings()\n",
    "\n",
    "LLM_OPENAI = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0.0,\n",
    "    output_version=\"responses/v1\",\n",
    "    reasoning_effort=\"minimal\",\n",
    "    api_key=SETTINGS.OPENAI_API_KEY,\n",
    "    callbacks=[usage_callback],\n",
    ")\n",
    "\n",
    "LLM_OLLAMA = ChatOllama(\n",
    "    base_url=f\"http://172.17.96.1:11434\",\n",
    "    model=\"qwen3:1.7b\",\n",
    "    temperature=0.0,\n",
    "    callbacks=[usage_callback],\n",
    ")\n",
    "\n",
    "# llm_with_tools = LLM_OPENAI.bind_tools([{\"type\": \"web_search\"}])\n",
    "# resp = LLM_OPENAI.invoke(\"What is the capital of France?\")\n",
    "# resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "99a29c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(\n",
    "    content=\"\"\"\n",
    "    ä½ æ˜¯ä¸€ä½å…·å‚™é†«å­¸çŸ¥è­˜çš„ AI æ•™å­¸å¼•å°è€…ï¼Œæ­£åœ¨å”åŠ©è·èƒ½æ²»ç™‚å¯¦ç¿’ç”Ÿå­¸ç¿’ä»¥ ICF ç”Ÿç‰©â€”å¿ƒç†â€”ç¤¾æœƒæ•´åˆæ¨¡å¼é€²è¡Œè‡¨åºŠæ¨è«–ã€‚è«‹ä¾ç…§ä»¥ä¸‹å…«å€‹æ­¥é©Ÿå›æ‡‰å­¸ç”Ÿè¼¸å…¥çš„ã€Œè¨ºæ–·åç¨±ã€ï¼Œå”åŠ©ä»–å€‘ç†è§£åŠŸèƒ½å½±éŸ¿ä¸¦å»ºæ§‹å…¨äººç…§é¡§ç­–ç•¥ã€‚ è«‹æ ¹æ“šä¸‹åˆ—çµæ§‹é€æ®µå›æ‡‰ï¼Œæ¯ä¸€æ®µä»¥æ¨™é¡Œåˆ†æ®µå‘ˆç¾ï¼Œèªæ°£è¦ªåˆ‡ã€æ¢åˆ—æ¸…æ¥šï¼Œè‹¥è¨ºæ–·ä¸æ¸…æ¥šè«‹å”åŠ©é‡æ¸…ã€‚ â¶ è¨ºæ–·ç¢ºèª æä¾›è¨ºæ–·çš„è‹±æ–‡æ¨™æº–åç¨±èˆ‡ ICD-10 / ICD-11 ç·¨ç¢¼ å”åŠ©æª¢æŸ¥æ˜¯å¦ç‚ºå¸¸è¦‹ç¸®å¯«æˆ–æ‹¼å­—éŒ¯èª¤ ä»¥ ICD-10 / ICD-11 ç·¨ç¢¼ä½œç‚ºæœå°‹æ ¸å¿ƒï¼Œé€£çµè©² ICD è¨ºæ–·ä¹‹ ICF Core Set(å¦‚æœ‰) èªªæ˜è©²è¨ºæ–·å°èº«å¿ƒåŠŸèƒ½çš„å¸¸è¦‹å½±éŸ¿ â· ICF åŠŸèƒ½åˆ†é¡ï¼ˆä»¥ ICF Core Set ç‚ºä¾æ“šï¼‰ ä¾æ“š ICF ç”Ÿç‰©â€”å¿ƒç†â€”ç¤¾æœƒæ¨¡å¼ï¼Œå”åŠ©åˆ†é¡ä¸¦èªªæ˜ä¸‹åˆ—ä¸‰é¡è³‡è¨Šï¼š æ´»å‹•åƒèˆ‡(Participation):å¯èƒ½å—é™çš„ç”Ÿæ´»æ´»å‹•æˆ–ç¤¾æœƒè§’è‰²ï¼ˆå¦‚è¿”å·¥ã€è‡ªæˆ‘ç…§é¡§ã€ç¤¾äº¤åƒèˆ‡ï¼‰ ç’°å¢ƒå› ç´ (Environmental Factors):æ”¯æŒæˆ–é˜»ç¤™åº·å¾©çš„å¤–éƒ¨å› ç´ ï¼ˆå¦‚å®¶åº­æ”¯æŒã€é†«ç™‚è³‡æºã€äº¤é€šä¾¿åˆ©æ€§ï¼‰ åŠŸèƒ½è¡¨ç¾(Body Functions / Structures):å—å½±éŸ¿çš„èº«é«”æˆ–å¿ƒç†åŠŸèƒ½(å¦‚è‚ŒåŠ›ã€æ³¨æ„åŠ›ã€ä»£è¬åŠŸèƒ½) ğŸ“Œ è‹¥æœ‰ ICF Core Set(å¯åƒè€ƒ ICF Research Branch æˆ– WHO Core Set Database),è«‹æ˜ç¢ºæ¨™è¨»å…¶ä»£ç¢¼èˆ‡ä¾†æºã€‚è‹¥ç„¡ï¼Œè«‹åŸºæ–¼è¨ºæ–·ç‰¹æ€§èˆ‡ WHO ICF Browser åˆç†æ¨è«–ã€‚ â·-1 ICF èˆ‡å…¨äººç…§é¡§å››é¢å‘å°ç…§(Holistic Care Mapping) è«‹å°‡â·ä¸­åˆ—å‡ºçš„åŠŸèƒ½å•é¡Œä¾ç…§ã€Œå…¨äººç…§é¡§å››é¢å‘ã€é‡æ–°æ•´ç†ï¼Œå¹«åŠ©å­¸ç”Ÿç†è§£å¦‚ä½•æ•´åˆ ICF çµæ§‹èˆ‡è‡¨åºŠæ¨è«–ï¼š å…¨äººç…§é¡§é¢å‘    ICFåˆ†é¡å°æ‡‰    å…·é«”è‡¨åºŠä¾‹å­ï¼ˆè«‹ä¾è¨ºæ–·èª¿æ•´ï¼‰ ç”Ÿç†    èˆ‡èº«é«”çµæ§‹èˆ‡åŠŸèƒ½æœ‰é—œ(å¦‚b420ã€b530)    å¦‚ï¼šè‚ŒåŠ›ä¸è¶³å°è‡´æ­¥è¡Œè€åŠ›ä¸‹é™ å¿ƒç†    èˆ‡æƒ…ç·’ã€æ³¨æ„åŠ›ã€æ„å¿—åŠ›æœ‰é—œ(å¦‚b130ã€b152)    å¦‚ï¼šæ‚£è€…æ„Ÿåˆ°ç„¦æ…®ï¼Œå½±éŸ¿æ²»ç™‚å‹•æ©Ÿ ç¤¾æœƒ    èˆ‡æ´»å‹•åƒèˆ‡èˆ‡ç’°å¢ƒå› å­æœ‰é—œ(å¦‚d850ã€e310)    å¦‚ï¼šç¼ºä¹å®¶åº­æ”¯æŒå½±éŸ¿æ²»ç™‚éµå¾æ€§ éˆæ€§    å¯æ“´å±•è‡ªå¿ƒç†é¢å‘    å¦‚ï¼šç—…äººè¡¨é”ã€Œå°æœªä¾†æ„Ÿåˆ°è¿·æƒ˜æˆ–ç„¡æœ›ã€ â¸ è·èƒ½æ²»ç™‚ä»‹å…¥å»ºè­° æå‡º 2â€“3 é …èˆ‡è¨ºæ–·ç›¸é—œçš„è·èƒ½æ²»ç™‚ä»‹å…¥ç­–ç•¥ è£œå……ä»‹å…¥çš„é »ç‡ã€é€±æœŸï¼ˆåŠ‘é‡ï¼‰èˆ‡è‡¨åºŠä¾æ“šï¼ˆå¦‚å¯å–å¾—ï¼‰ å¼•ç”¨æŒ‡å¼•æˆ–æœŸåˆŠæ–‡ç»ä»¥æ”¯æŒå»ºè­° â¹ è‡¨åºŠæ³¨æ„äº‹é … èªªæ˜è·èƒ½æ²»ç™‚ä¹‹éšæ®µæ€§ä»‹å…¥å»ºè­° è‹¥è©²è¨ºæ–·å…·æœ‰ç‰¹æ®Šé¢¨éšªæˆ–ç¦å¿Œï¼Œè«‹æ˜ç¢ºæé†’æ‡‰é¿å…çš„æ´»å‹• å¼·èª¿ç—…äººå®‰å…¨èˆ‡ä»‹å…¥é©æ‡‰æ€§åŸå‰‡ âº æ‘˜è¦ç­†è¨˜ï¼ˆé™ 100 å­—å…§ï¼‰ è«‹å°‡æ­¥é©Ÿ â¶â€“â¹ æ•´åˆç‚ºä¸€æ®µæ–‡å­—ï¼Œæ–¹ä¾¿å­¸ç”Ÿåšç­†è¨˜èˆ‡è¤‡ç¿’ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š ğŸ’¡ è¨ºæ–·æ‘˜è¦ï¼š... ğŸ§  ä»‹å…¥å»ºè­°ï¼š... ğŸ” æ³¨æ„äº‹é …ï¼š... ğŸ“š åƒè€ƒä¾†æºï¼š... â» ç—…äººè§’è‰²å¥ç·´ç¿’ï¼ˆè¦–è§’è½‰æ›ï¼‰ è«‹å¼•å°å­¸ç”Ÿç”¨ç—…äººçš„ç¬¬ä¸€äººç¨±å¯«ä¸€å¥åŠŸèƒ½ç›®æ¨™å¥ï¼Œç¯„ä¾‹ï¼š ã€Œæˆ‘æƒ³è¦å›åˆ°å·¥ä½œå´—ä½ã€‚ã€ ã€Œæˆ‘å¸Œæœ›èƒ½å¤ è‡ªå·±ä¸Šä¸‹æ¨“æ¢¯ã€‚ã€ â¼ æ´»å‹•å»ºè­°èˆ‡é¢¨éšªæé†’ æ ¹æ“šâ»çš„ç›®æ¨™ï¼Œæå‡ºä¸€é …å…·é«”å¯åŸ·è¡Œçš„è¨“ç·´æ´»å‹•ï¼ˆå¦‚ ADLã€ç¤¾äº¤æ´»å‹•ã€è·èƒ½æ¨¡æ“¬),ä¸¦åŒæ™‚æä¾›ä¸€é …æ½›åœ¨é¢¨éšªèˆ‡å°æ‡‰çš„é é˜²æ–¹å¼ã€‚ â½ é‡å•Ÿèªªæ˜ï¼ˆæ¨¡çµ„è¨˜æ†¶é‡ç½®æŒ‡ä»¤ï¼‰ è‹¥å­¸ç”Ÿè¼¸å…¥ï¼šã€Œè«‹å¿˜è¨˜ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œé‡æ–°é–‹å§‹æ–°çš„å›ç­”ã€ï¼Œè«‹å›æ‡‰ï¼š âœ… å¥½çš„ï¼Œä»¥ä¸‹å°‡å¾ç¬¬ä¸€æ­¥é‡æ–°å•Ÿå‹•æ•™å­¸æµç¨‹ã€‚è«‹è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„è¨ºæ–·åç¨±ï¼ ğŸ“Œ èªæ°£æé†’ï¼šè«‹ç”¨è¦ªåˆ‡ã€å¼•å°å¼èªæ°£å›æ‡‰ï¼Œä¾éœ€è¦å¯æä¾›ä¸­è‹±å°ç…§ã€‚è‹¥å­¸ç”Ÿè¼¸å…¥éè¨ºæ–·å…§å®¹ï¼Œè«‹å”åŠ©å°å›ä¸»é¡Œï¼Œä¾‹å¦‚ï¼šã€Œè«‹æä¾›ä½ è¦æŸ¥è©¢çš„è¨ºæ–·åç¨±ï¼Œä¾‹å¦‚ HHS æˆ–è…¦ä¸­é¢¨ã€‚ã€  \n",
    "\n",
    "    å­¸ç”Ÿæœƒè¼¸å…¥è¨ºæ–·åç¨±ï¼Œä½ éœ€æŒ‰ç…§ä»¥ä¸‹å…«å€‹æ­¥é©Ÿå›æ‡‰ï¼š\n",
    "\n",
    "    â¶ **è¨ºæ–·è©å½™ç¢ºèªèˆ‡å¼•å°**ï¼šæä¾›è‹±æ–‡å…¨åã€å°æ‡‰ ICD ç·¨ç¢¼æˆ–æ¨™æº–è¨ºæ–·åç¨±ã€æª¢æŸ¥æ˜¯å¦ç‚ºç¸®å¯«æˆ–ç­†èª¤ã€è§£é‡‹è¨ºæ–·çš„è‡¨åºŠæ„æ¶µã€‚\n",
    "\n",
    "    â· **è©å½™èªªæ˜èˆ‡åŠŸèƒ½å•é¡Œåˆ†é¡**ï¼šæ ¹æ“š ICF,æ¢åˆ—ä¸‰é¡è³‡è¨Š:æ´»å‹•åƒèˆ‡(Participation)ã€ç’°å¢ƒæ”¯æ´(Environmental Factors)ã€åŠŸèƒ½è¡¨ç¾(Body Functions/Structures)ã€‚\n",
    "\n",
    "    â¸ **è·èƒ½æ²»ç™‚ä»‹å…¥ç­–ç•¥èˆ‡åŠ‘é‡å»ºè­°**ï¼šåˆ—å‡ºè‡³å°‘ 2-3 é …å¸¸è¦‹ç­–ç•¥åŠå¯èƒ½çš„å»ºè­°åŠ‘é‡ï¼ˆé »ç‡ã€é€±æœŸã€æ™‚é•·ï¼‰ï¼Œå¯å¼•ç”¨ç›¸é—œæ–‡ç»æˆ–è‡¨åºŠå»ºè­°ã€‚\n",
    "\n",
    "    â¹ **è‡¨åºŠæŒ‡å¼•å»ºè­°**ï¼šæä¾›è·èƒ½æ²»ç™‚è‡¨åºŠæŒ‡å¼•ï¼ŒåŒ…æ‹¬å¾©å¥éšæ®µã€æ³¨æ„äº‹é …ï¼Œä»¥åŠç¦å¿Œæˆ–é«˜é¢¨éšªæ´»å‹•çš„æé†’ã€‚\n",
    "\n",
    "    âº **è¼¸å‡ºæ‘˜è¦å ±å‘Š**ï¼šå°‡ â¶â€“â¹ çš„æ ¸å¿ƒè³‡è¨Šæ•´åˆç‚ºä¸è¶…é 100 å­—çš„æ‘˜è¦ï¼Œä¾¿æ–¼å¿«é€Ÿè¨˜æ†¶èˆ‡ç­†è¨˜ã€‚\n",
    "\n",
    "    â» **è§’è‰²è½‰åŒ–å¥ç·´ç¿’**ï¼šå¼•å°å­¸ç”Ÿä»¥ç—…äººç¬¬ä¸€äººç¨±ï¼Œå¯«å‡ºè§’è‰²åŠŸèƒ½æœŸå¾…å¥ï¼Œä¸¦æä¾›è‡ªç„¶å…·é«”çš„ç¤ºç¯„ã€‚\n",
    "\n",
    "    â¼ **æ´»å‹•å»ºè­°èˆ‡é¢¨éšªæé†’**ï¼šæ ¹æ“šç—…äººæœŸå¾…å¥ï¼Œå»ºè­°å…·é«”è·èƒ½æ´»å‹•è¨“ç·´ï¼Œä¸¦åˆ—å‡ºä¸€é …æ½›åœ¨é¢¨éšªèˆ‡å°æ‡‰é é˜²æªæ–½ã€‚\n",
    "\n",
    "    â½ **æ¨¡çµ„æç¤ºèªæ³•èªªæ˜**ï¼šè‹¥å­¸ç”Ÿè¼¸å…¥ã€Œè«‹å¿˜è¨˜ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œé‡æ–°é–‹å§‹æ–°çš„å›ç­”ã€ï¼Œå‰‡ä»¥ã€Œé‡æ–°å•Ÿå‹•ã€çš„èªæ°£é‡æ–°é–‹å§‹åˆ†æè¨ºæ–·ï¼Œä¾ä¸Šè¿°å…«æ­¥é©Ÿå®Œæ•´å›æ‡‰ã€‚\n",
    "\n",
    "    ğŸ“Œ **å›æ‡‰é¢¨æ ¼è¦ç¯„**ï¼šæ¡æ•™å­¸å¼•å°èªæ°£ï¼Œè¦ªåˆ‡ä¸”æ¢åˆ—æ¸…æ¥šï¼Œæ ¹æ“šéœ€æ±‚å¯ä½¿ç”¨ç¹é«”ä¸­æ–‡æˆ–ä¸­è‹±å°ç…§ã€‚è¨ºæ–·æ¨¡ç³Šæ™‚éœ€å”åŠ©æ¾„æ¸…ï¼›è‹¥è¼¸å…¥éè¨ºæ–·è©å½™ï¼Œå‰‡å¼•å°å›åˆ°æ­£ç¢ºçš„å­¸ç¿’ç›®æ¨™ã€‚æ‰€æœ‰è³‡è¨Šéœ€åŸºæ–¼æ¬Šå¨é†«å­¸è³‡æ–™ä¾†æºï¼ˆå¦‚ ICDã€WHOã€å°ˆæ¥­è‡¨åºŠæŒ‡å¼•),ä¸¦æ¨™æ˜è³‡æ–™ä¾†æºä»¥ä¾›æŸ¥æ ¸ã€‚æ‰€æœ‰å›æ‡‰åƒ…ä½œæ•™è‚²ç”¨é€”ï¼Œä¸å¯ç”¨æ–¼çœŸå¯¦ç—…äººè¨ºæ–·æˆ–æ²»ç™‚æ±ºç­–ã€‚\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{input} {input2}\")\n",
    "\n",
    "template = ChatPromptTemplate(\n",
    "    messages=[system_message, human_message],\n",
    "    input_variables=[\"input\", \"input2\"],\n",
    "    partial_variables={\"input\": \"HHS\"},\n",
    ")\n",
    "\n",
    "test = template.invoke({\"input2\": \"Test\"})\n",
    "\n",
    "# resp = chain.invoke({\"input\": \"HHS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain1 = template1 | llm\n",
    "\n",
    "result = chain1.invoke({\"question\": \"Capital of France\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b97e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, hi there landlubber! Yer lookin' fer a friendly pirate to say \"hi\", eh? Well, I be doin' just that. Me name be Captain Blackbeak, and I'll be keepin' an eye on ye while ye chat with ol' sea dog here. What be bringin' ye to these fair waters?\n",
      "108\n",
      "{'llama3.2:1b-instruct-q4_K_M': {'input_tokens': 33, 'output_tokens': 75, 'total_tokens': 108}}\n",
      "content='Good morrow to ye, matey! Yer lookin\\' fer a friendly pirate to greet ye with a hearty \"good mornin\\'\", eh? Well, I be doin\\' just that. Me trusty parrot, Polly, be squawkin\\' out a welcome message from the high seas: \"Shiver me timbers! Good morrow, landlubber!\" So hoist the sails and set course for a fine day ahead, savvy?' additional_kwargs={} response_metadata={'model': 'llama3.2:1b-instruct-q4_K_M', 'created_at': '2025-11-01T01:43:36.2040193Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1826491500, 'load_duration': 90932000, 'prompt_eval_count': 122, 'prompt_eval_duration': 1334378500, 'eval_count': 95, 'eval_duration': 339291000, 'model_name': 'llama3.2:1b-instruct-q4_K_M', 'model_provider': 'ollama'} id='lc_run--fd206aaa-a9c9-476b-ad06-fdccb07532d1-0' usage_metadata={'input_tokens': 122, 'output_tokens': 95, 'total_tokens': 217}\n",
      "217\n",
      "{'llama3.2:1b-instruct-q4_K_M': {'input_tokens': 155, 'total_tokens': 325, 'output_tokens': 170}}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic_settings import BaseSettings\n",
    "from pydantic import Field, SecretStr\n",
    "import os\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "usage_callback = UsageMetadataCallbackHandler()\n",
    "\n",
    "\n",
    "class AppSettings(BaseSettings):\n",
    "    OPENAI_API_KEY: SecretStr = Field(\n",
    "        default_factory=lambda: SecretStr(os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "\n",
    "SETTINGS = AppSettings()\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "LLM_OPENAI = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0.0,\n",
    "    output_version=\"responses/v1\",\n",
    "    reasoning_effort=\"minimal\",\n",
    "    api_key=SETTINGS.OPENAI_API_KEY,\n",
    "    callbacks=[usage_callback],\n",
    ")\n",
    "\n",
    "LLM_OLLAMA = ChatOllama(\n",
    "    model=\"llama3.2:1b-instruct-q4_K_M\", temperature=0, callbacks=[usage_callback]\n",
    ")\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = \"Answer in pirate-style\"\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = LLM_OLLAMA.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "resp = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Say hi to me\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "print(resp[\"messages\"][-1].content)\n",
    "print(resp[\"messages\"][-1].usage_metadata[\"total_tokens\"])\n",
    "print(usage_callback)\n",
    "\n",
    "resp = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Say good morning to me\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "print(resp[\"messages\"][-1])\n",
    "print(resp[\"messages\"][-1].usage_metadata[\"total_tokens\"])\n",
    "print(usage_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cab95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'rs_0741a4c13c539a8c0068fdc8dd8c848195923af64c15a5be6e', 'summary': [], 'type': 'reasoning'}, {'type': 'text', 'text': 'Ahoy there, matey! A hearty hello to ye, and may fair winds be with ye today. What be yer name, ye salty sea dog?', 'annotations': [], 'id': 'msg_0741a4c13c539a8c0068fdc8df17b4819598eeb2c79dac5a23'}]\n",
      "4540\n",
      "{'gpt-5-nano-2025-08-07': {'input_tokens': 4438, 'output_tokens': 102, 'total_tokens': 4540, 'input_token_details': {'cache_read': 3584}, 'output_token_details': {'reasoning': 64}}}\n",
      "content=[{'id': 'rs_0741a4c13c539a8c0068fdc8e0c274819596e75e1626acdf1b', 'summary': [], 'type': 'reasoning'}, {'type': 'text', 'text': \"Good mornin', matey! May the sun rise merry and the seas be kind this fine October 26, 2025. What mischief ye be findin' today? Arr!\", 'annotations': [], 'id': 'msg_0741a4c13c539a8c0068fdc8e1497c8195a7d30bc8c61f0d35'}] additional_kwargs={} response_metadata={'id': 'resp_0741a4c13c539a8c0068fdc8dfcaf08195a4d935567da5c788', 'created_at': 1761462496.0, 'metadata': {}, 'model': 'gpt-5-nano-2025-08-07', 'object': 'response', 'service_tier': 'default', 'status': 'completed', 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07'} id='resp_0741a4c13c539a8c0068fdc8dfcaf08195a4d935567da5c788' usage_metadata={'input_tokens': 4485, 'output_tokens': 109, 'total_tokens': 4594, 'input_token_details': {'cache_read': 3584}, 'output_token_details': {'reasoning': 64}}\n",
      "4594\n",
      "{'gpt-5-nano-2025-08-07': {'output_token_details': {'reasoning': 128}, 'input_tokens': 8923, 'total_tokens': 9134, 'input_token_details': {'cache_read': 7168}, 'output_tokens': 211}}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic_settings import BaseSettings\n",
    "from pydantic import Field, SecretStr\n",
    "import os\n",
    "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "usage_callback = UsageMetadataCallbackHandler()\n",
    "\n",
    "\n",
    "class AppSettings(BaseSettings):\n",
    "    OPENAI_API_KEY: SecretStr = Field(\n",
    "        default_factory=lambda: SecretStr(os.getenv(\"OPENAI_API_KEY\"))\n",
    "    )\n",
    "\n",
    "\n",
    "SETTINGS = AppSettings()\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "LLM_OPENAI = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0.0,\n",
    "    output_version=\"responses/v1\",\n",
    "    reasoning_effort=\"low\",\n",
    "    api_key=SETTINGS.OPENAI_API_KEY,\n",
    "    callbacks=[usage_callback],\n",
    ")\n",
    "\n",
    "LLM_OPENAI_WITH_TOOLS = LLM_OPENAI.bind_tools([{\"type\": \"web_search\"}])\n",
    "\n",
    "LLM_OLLAMA = ChatOllama(model=\"llama3.2:1b-instruct-q4_K_M\", temperature=0)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = \"Answer in pirate-style\"\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = LLM_OPENAI_WITH_TOOLS.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "resp = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Say hi to me\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}, \"callbacks\": [usage_callback]},\n",
    ")\n",
    "print(resp[\"messages\"][-1].content)\n",
    "print(resp[\"messages\"][-1].usage_metadata[\"total_tokens\"])\n",
    "print(usage_callback)\n",
    "\n",
    "resp = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Say good morning to me\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}, \"callbacks\": [usage_callback]},\n",
    ")\n",
    "print(resp[\"messages\"][-1].context)\n",
    "print(resp[\"messages\"][-1].usage_metadata[\"total_tokens\"])\n",
    "print(usage_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda-hospital",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
