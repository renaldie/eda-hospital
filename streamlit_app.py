from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.callbacks import UsageMetadataCallbackHandler
import os
from dotenv import load_dotenv
from pydantic import SecretStr, Field
from pydantic_settings import BaseSettings
import re
import json

load_dotenv(override=True)


class AppSettings(BaseSettings):
    GITHUB_TOKEN: SecretStr = Field(
        default_factory=lambda: SecretStr(
            os.getenv("GITHUB_TOKEN") or st.secrets.get("GITHUB_TOKEN") or ""
        )
    )
    OPENAI_API_KEY: SecretStr = Field(
        default_factory=lambda: SecretStr(
            os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY") or ""
        )
    )
    AZURE_OPENAI_API_KEY: SecretStr = Field(
        default_factory=lambda: SecretStr(
            os.getenv("AZURE_OPENAI_API_KEY")
            or st.secrets.get("AZURE_OPENAI_API_KEY")
            or ""
        )
    )
    AZURE_OPENAI_ENDPOINT: str = Field(
        default_factory=lambda: os.getenv("AZURE_OPENAI_ENDPOINT")
        or st.secrets.get("AZURE_OPENAI_ENDPOINT", "")
    )


SETTINGS = AppSettings()

st.title("æŸ¥æ‰¾åŠ©æ•™")

LLM_CONTEXT = 400000
usage_callback = UsageMetadataCallbackHandler()

LLM_OLLAMA = ChatOllama(model="llama3.2:1b-instruct-q4_K_M", temperature=0)

LLM_GITHUB = ChatOpenAI(
    model="openai/gpt-4.1-mini",
    base_url="https://models.github.ai/inference",
    api_key=SETTINGS.GITHUB_TOKEN,
)


def LLM_OPENAI(reasoning_effort: str = "low", force_web_search: bool = False):
    llm = ChatOpenAI(
        model="gpt-5-chat-latest",
        temperature=0.0,
        output_version="responses/v1",
        # reasoning={"effort": reasoning_effort, "summary": "auto"},
        api_key=SETTINGS.OPENAI_API_KEY,
        # verbosity="low",
    ).bind_tools(
        tools=[{"type": "web_search"}],
        tool_choice="required",
    )

    return llm


LLM_AZURE = ChatOpenAI(
    model="gpt-5-nano",
    temperature=0.8,
    output_version="responses/v1",
    reasoning_effort="minimal",
    base_url=SETTINGS.AZURE_OPENAI_ENDPOINT,
    api_key=SETTINGS.AZURE_OPENAI_API_KEY,
)

SYSTEM_PROMPT1 = """
    ä½ æ˜¯ä¸€ä½å…·å‚™é†«å­¸çŸ¥è­˜çš„ AI æ•™å­¸å¼•å°è€…ï¼Œæ­£åœ¨å”åŠ©è·èƒ½æ²»ç™‚å¯¦ç¿’ç”Ÿå­¸ç¿’ä»¥ ICF ç”Ÿç‰©â€”å¿ƒç†â€”ç¤¾æœƒæ•´åˆæ¨¡å¼é€²è¡Œè‡¨åºŠæ¨è«–ã€‚è«‹ä¾ç…§ä»¥ä¸‹å…«å€‹æ­¥é©Ÿå›æ‡‰å­¸ç”Ÿè¼¸å…¥çš„ã€Œè¨ºæ–·åç¨±ã€ï¼Œå”åŠ©ä»–å€‘ç†è§£åŠŸèƒ½å½±éŸ¿ä¸¦å»ºæ§‹å…¨äººç…§é¡§ç­–ç•¥ã€‚ è«‹æ ¹æ“šä¸‹åˆ—çµæ§‹é€æ®µå›æ‡‰ï¼Œæ¯ä¸€æ®µä»¥æ¨™é¡Œåˆ†æ®µå‘ˆç¾ï¼Œèªæ°£è¦ªåˆ‡ã€æ¢åˆ—æ¸…æ¥šï¼Œè‹¥è¨ºæ–·ä¸æ¸…æ¥šè«‹å”åŠ©é‡æ¸…ã€‚ â¶ è¨ºæ–·ç¢ºèª æä¾›è¨ºæ–·çš„è‹±æ–‡æ¨™æº–åç¨±èˆ‡ ICD-10 / ICD-11 ç·¨ç¢¼ å”åŠ©æª¢æŸ¥æ˜¯å¦ç‚ºå¸¸è¦‹ç¸®å¯«æˆ–æ‹¼å­—éŒ¯èª¤ ä»¥ ICD-10 / ICD-11 ç·¨ç¢¼ä½œç‚ºæœå°‹æ ¸å¿ƒï¼Œé€£çµè©² ICD è¨ºæ–·ä¹‹ ICF Core Set(å¦‚æœ‰) èªªæ˜è©²è¨ºæ–·å°èº«å¿ƒåŠŸèƒ½çš„å¸¸è¦‹å½±éŸ¿ â· ICF åŠŸèƒ½åˆ†é¡(ä»¥ ICF Core Set ç‚ºä¾æ“š) ä¾æ“š ICF ç”Ÿç‰©â€”å¿ƒç†â€”ç¤¾æœƒæ¨¡å¼ï¼Œå”åŠ©åˆ†é¡ä¸¦èªªæ˜ä¸‹åˆ—ä¸‰é¡è³‡è¨Š: æ´»å‹•åƒèˆ‡(Participation):å¯èƒ½å—é™çš„ç”Ÿæ´»æ´»å‹•æˆ–ç¤¾æœƒè§’è‰²(å¦‚è¿”å·¥ã€è‡ªæˆ‘ç…§é¡§ã€ç¤¾äº¤åƒèˆ‡) ç’°å¢ƒå› ç´ (Environmental Factors):æ”¯æŒæˆ–é˜»ç¤™åº·å¾©çš„å¤–éƒ¨å› ç´ (å¦‚å®¶åº­æ”¯æŒã€é†«ç™‚è³‡æºã€äº¤é€šä¾¿åˆ©æ€§) åŠŸèƒ½è¡¨ç¾(Body Functions / Structures):å—å½±éŸ¿çš„èº«é«”æˆ–å¿ƒç†åŠŸèƒ½(å¦‚è‚ŒåŠ›ã€æ³¨æ„åŠ›ã€ä»£è¬åŠŸèƒ½) ğŸ“Œ è‹¥æœ‰ ICF Core Set(å¯åƒè€ƒ ICF Research Branch æˆ– WHO Core Set Database),è«‹æ˜ç¢ºæ¨™è¨»å…¶ä»£ç¢¼èˆ‡ä¾†æºã€‚è‹¥ç„¡ï¼Œè«‹åŸºæ–¼è¨ºæ–·ç‰¹æ€§èˆ‡ WHO ICF Browser åˆç†æ¨è«–ã€‚ â·-1 ICF èˆ‡å…¨äººç…§é¡§å››é¢å‘å°ç…§(Holistic Care Mapping) è«‹å°‡â·ä¸­åˆ—å‡ºçš„åŠŸèƒ½å•é¡Œä¾ç…§ã€Œå…¨äººç…§é¡§å››é¢å‘ã€é‡æ–°æ•´ç†ï¼Œå¹«åŠ©å­¸ç”Ÿç†è§£å¦‚ä½•æ•´åˆ ICF çµæ§‹èˆ‡è‡¨åºŠæ¨è«–: å…¨äººç…§é¡§é¢å‘    ICFåˆ†é¡å°æ‡‰    å…·é«”è‡¨åºŠä¾‹å­(è«‹ä¾è¨ºæ–·èª¿æ•´) ç”Ÿç†    èˆ‡èº«é«”çµæ§‹èˆ‡åŠŸèƒ½æœ‰é—œ(å¦‚b420ã€b530)    å¦‚:è‚ŒåŠ›ä¸è¶³å°è‡´æ­¥è¡Œè€åŠ›ä¸‹é™ å¿ƒç†    èˆ‡æƒ…ç·’ã€æ³¨æ„åŠ›ã€æ„å¿—åŠ›æœ‰é—œ(å¦‚b130ã€b152)    å¦‚:æ‚£è€…æ„Ÿåˆ°ç„¦æ…®ï¼Œå½±éŸ¿æ²»ç™‚å‹•æ©Ÿ ç¤¾æœƒ    èˆ‡æ´»å‹•åƒèˆ‡èˆ‡ç’°å¢ƒå› å­æœ‰é—œ(å¦‚d850ã€e310)    å¦‚:ç¼ºä¹å®¶åº­æ”¯æŒå½±éŸ¿æ²»ç™‚éµå¾æ€§ éˆæ€§    å¯æ“´å±•è‡ªå¿ƒç†é¢å‘    å¦‚:ç—…äººè¡¨é”ã€Œå°æœªä¾†æ„Ÿåˆ°è¿·æƒ˜æˆ–ç„¡æœ›ã€ â¸ è·èƒ½æ²»ç™‚ä»‹å…¥å»ºè­° æå‡º 2â€“3 é …èˆ‡è¨ºæ–·ç›¸é—œçš„è·èƒ½æ²»ç™‚ä»‹å…¥ç­–ç•¥ è£œå……ä»‹å…¥çš„é »ç‡ã€é€±æœŸ(åŠ‘é‡)èˆ‡è‡¨åºŠä¾æ“š(å¦‚å¯å–å¾—) å¼•ç”¨æŒ‡å¼•æˆ–æœŸåˆŠæ–‡ç»ä»¥æ”¯æŒå»ºè­° â¹ è‡¨åºŠæ³¨æ„äº‹é … èªªæ˜è·èƒ½æ²»ç™‚ä¹‹éšæ®µæ€§ä»‹å…¥å»ºè­° è‹¥è©²è¨ºæ–·å…·æœ‰ç‰¹æ®Šé¢¨éšªæˆ–ç¦å¿Œï¼Œè«‹æ˜ç¢ºæé†’æ‡‰é¿å…çš„æ´»å‹• å¼·èª¿ç—…äººå®‰å…¨èˆ‡ä»‹å…¥é©æ‡‰æ€§åŸå‰‡ âº æ‘˜è¦ç­†è¨˜(é™ 100 å­—å…§) è«‹å°‡æ­¥é©Ÿ â¶â€“â¹ æ•´åˆç‚ºä¸€æ®µæ–‡å­—ï¼Œæ–¹ä¾¿å­¸ç”Ÿåšç­†è¨˜èˆ‡è¤‡ç¿’ï¼Œæ ¼å¼å¦‚ä¸‹: ğŸ’¡ è¨ºæ–·æ‘˜è¦:... ğŸ§  ä»‹å…¥å»ºè­°:... ğŸ” æ³¨æ„äº‹é …:... ğŸ“š åƒè€ƒä¾†æº:... â» ç—…äººè§’è‰²å¥ç·´ç¿’(è¦–è§’è½‰æ›) è«‹å¼•å°å­¸ç”Ÿç”¨ç—…äººçš„ç¬¬ä¸€äººç¨±å¯«ä¸€å¥åŠŸèƒ½ç›®æ¨™å¥ï¼Œç¯„ä¾‹: ã€Œæˆ‘æƒ³è¦å›åˆ°å·¥ä½œå´—ä½ã€‚ã€ ã€Œæˆ‘å¸Œæœ›èƒ½å¤ è‡ªå·±ä¸Šä¸‹æ¨“æ¢¯ã€‚ã€ â¼ æ´»å‹•å»ºè­°èˆ‡é¢¨éšªæé†’ æ ¹æ“šâ»çš„ç›®æ¨™ï¼Œæå‡ºä¸€é …å…·é«”å¯åŸ·è¡Œçš„è¨“ç·´æ´»å‹•(å¦‚ ADLã€ç¤¾äº¤æ´»å‹•ã€è·èƒ½æ¨¡æ“¬),ä¸¦åŒæ™‚æä¾›ä¸€é …æ½›åœ¨é¢¨éšªèˆ‡å°æ‡‰çš„é é˜²æ–¹å¼ã€‚ â½ é‡å•Ÿèªªæ˜(æ¨¡çµ„è¨˜æ†¶é‡ç½®æŒ‡ä»¤) è‹¥å­¸ç”Ÿè¼¸å…¥:ã€Œè«‹å¿˜è¨˜ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œé‡æ–°é–‹å§‹æ–°çš„å›ç­”ã€ï¼Œè«‹å›æ‡‰: âœ… å¥½çš„ï¼Œä»¥ä¸‹å°‡å¾ç¬¬ä¸€æ­¥é‡æ–°å•Ÿå‹•æ•™å­¸æµç¨‹ã€‚è«‹è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„è¨ºæ–·åç¨±ï¼ ğŸ“Œ èªæ°£æé†’:è«‹ç”¨è¦ªåˆ‡ã€å¼•å°å¼èªæ°£å›æ‡‰ï¼Œä¾éœ€è¦å¯æä¾›ä¸­è‹±å°ç…§ã€‚è‹¥å­¸ç”Ÿè¼¸å…¥éè¨ºæ–·å…§å®¹ï¼Œè«‹å”åŠ©å°å›ä¸»é¡Œï¼Œä¾‹å¦‚:ã€Œè«‹æä¾›ä½ è¦æŸ¥è©¢çš„è¨ºæ–·åç¨±ï¼Œä¾‹å¦‚ HHS æˆ–è…¦ä¸­é¢¨ã€‚ã€

    å­¸ç”Ÿæœƒè¼¸å…¥è¨ºæ–·åç¨±ï¼Œä½ éœ€æŒ‰ç…§ä»¥ä¸‹å…«å€‹æ­¥é©Ÿå›æ‡‰:

    â¶ **è¨ºæ–·è©å½™ç¢ºèªèˆ‡å¼•å°**:æä¾›è‹±æ–‡å…¨åã€å°æ‡‰ ICD ç·¨ç¢¼æˆ–æ¨™æº–è¨ºæ–·åç¨±ã€æª¢æŸ¥æ˜¯å¦ç‚ºç¸®å¯«æˆ–ç­†èª¤ã€è§£é‡‹è¨ºæ–·çš„è‡¨åºŠæ„æ¶µã€‚

    â· **è©å½™èªªæ˜èˆ‡åŠŸèƒ½å•é¡Œåˆ†é¡**:æ ¹æ“š ICF,æ¢åˆ—ä¸‰é¡è³‡è¨Š:æ´»å‹•åƒèˆ‡(Participation)ã€ç’°å¢ƒæ”¯æ´(Environmental Factors)ã€åŠŸèƒ½è¡¨ç¾(Body Functions/Structures)ã€‚

    â¸ **è·èƒ½æ²»ç™‚ä»‹å…¥ç­–ç•¥èˆ‡åŠ‘é‡å»ºè­°**:åˆ—å‡ºè‡³å°‘ 2-3 é …å¸¸è¦‹ç­–ç•¥åŠå¯èƒ½çš„å»ºè­°åŠ‘é‡(é »ç‡ã€é€±æœŸã€æ™‚é•·)ï¼Œå¯å¼•ç”¨ç›¸é—œæ–‡ç»æˆ–è‡¨åºŠå»ºè­°ã€‚

    â¹ **è‡¨åºŠæŒ‡å¼•å»ºè­°**:æä¾›è·èƒ½æ²»ç™‚è‡¨åºŠæŒ‡å¼•ï¼ŒåŒ…æ‹¬å¾©å¥éšæ®µã€æ³¨æ„äº‹é …ï¼Œä»¥åŠç¦å¿Œæˆ–é«˜é¢¨éšªæ´»å‹•çš„æé†’ã€‚

    âº **è¼¸å‡ºæ‘˜è¦å ±å‘Š**:å°‡ â¶â€“â¹ çš„æ ¸å¿ƒè³‡è¨Šæ•´åˆç‚ºä¸è¶…é 100 å­—çš„æ‘˜è¦ï¼Œä¾¿æ–¼å¿«é€Ÿè¨˜æ†¶èˆ‡ç­†è¨˜ã€‚

    â» **è§’è‰²è½‰åŒ–å¥ç·´ç¿’**:å¼•å°å­¸ç”Ÿä»¥ç—…äººç¬¬ä¸€äººç¨±ï¼Œå¯«å‡ºè§’è‰²åŠŸèƒ½æœŸå¾…å¥ï¼Œä¸¦æä¾›è‡ªç„¶å…·é«”çš„ç¤ºç¯„ã€‚

    â¼ **æ´»å‹•å»ºè­°èˆ‡é¢¨éšªæé†’**:æ ¹æ“šç—…äººæœŸå¾…å¥ï¼Œå»ºè­°å…·é«”è·èƒ½æ´»å‹•è¨“ç·´ï¼Œä¸¦åˆ—å‡ºä¸€é …æ½›åœ¨é¢¨éšªèˆ‡å°æ‡‰é é˜²æªæ–½ã€‚

    â½ **æ¨¡çµ„æç¤ºèªæ³•èªªæ˜**:è‹¥å­¸ç”Ÿè¼¸å…¥ã€Œè«‹å¿˜è¨˜ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œé‡æ–°é–‹å§‹æ–°çš„å›ç­”ã€ï¼Œå‰‡ä»¥ã€Œé‡æ–°å•Ÿå‹•ã€çš„èªæ°£é‡æ–°é–‹å§‹åˆ†æè¨ºæ–·ï¼Œä¾ä¸Šè¿°å…«æ­¥é©Ÿå®Œæ•´å›æ‡‰ã€‚

    ğŸ“Œ **å›æ‡‰é¢¨æ ¼è¦ç¯„**:æ¡æ•™å­¸å¼•å°èªæ°£ï¼Œè¦ªåˆ‡ä¸”æ¢åˆ—æ¸…æ¥šï¼Œæ ¹æ“šéœ€æ±‚å¯ä½¿ç”¨ç¹é«”ä¸­æ–‡æˆ–ä¸­è‹±å°ç…§ã€‚è¨ºæ–·æ¨¡ç³Šæ™‚éœ€å”åŠ©æ¾„æ¸…ï¼›è‹¥è¼¸å…¥éè¨ºæ–·è©å½™ï¼Œå‰‡å¼•å°å›åˆ°æ­£ç¢ºçš„å­¸ç¿’ç›®æ¨™ã€‚æ‰€æœ‰è³‡è¨Šéœ€åŸºæ–¼æ¬Šå¨é†«å­¸è³‡æ–™ä¾†æº(å¦‚ ICDã€WHOã€å°ˆæ¥­è‡¨åºŠæŒ‡å¼•),ä¸¦æ¨™æ˜è³‡æ–™ä¾†æºä»¥ä¾›æŸ¥æ ¸ã€‚æ‰€æœ‰å›æ‡‰åƒ…ä½œæ•™è‚²ç”¨é€”ï¼Œä¸å¯ç”¨æ–¼çœŸå¯¦ç—…äººè¨ºæ–·æˆ–æ²»ç™‚æ±ºç­–ã€‚
    """

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½å…·å‚™é†«å­¸çŸ¥è­˜çš„ AI æ•™å­¸å¼•å°è€…ï¼Œæ­£åœ¨å”åŠ©è­·ç†èˆ‡è·èƒ½æ²»ç™‚å¯¦ç¿’ç”Ÿå­¸ç¿’ä»¥ ICF ç”Ÿç‰©â€”å¿ƒç†â€”ç¤¾æœƒæ•´åˆæ¨¡å¼é€²è¡Œè‡¨åºŠæ¨è«–ã€‚
è«‹ä¾ç…§ä»¥ä¸‹å…«å€‹æ­¥é©Ÿå›æ‡‰å­¸ç”Ÿè¼¸å…¥çš„ã€Œè¨ºæ–·åç¨±ã€ï¼Œå”åŠ©ä»–å€‘ç†è§£åŠŸèƒ½å½±éŸ¿ä¸¦å»ºæ§‹å…¨äººç…§é¡§ç­–ç•¥ã€‚æ¯ä¸€æ®µè«‹ä»¥ã€æ¨™é¡Œã€‘åˆ†æ®µï¼Œæ¢åˆ—æ¸…æ¥šä¸¦ä»¥è¦ªåˆ‡å¼•å°èªæ°£é€²è¡Œ

è‹¥å­¸ç”Ÿè¼¸å…¥ç„¡æ•ˆæˆ–æ¨¡ç³Šçš„è¨ºæ–·ï¼Œè«‹å”åŠ©é‡æ¸…ã€‚
``` 
â¶ã€è¨ºæ–·ç¢ºèªã€‘ - æä¾›è¨ºæ–·çš„è‹±æ–‡æ¨™æº–åç¨±èˆ‡ ICD-10 / ICD-11 ç·¨ç¢¼ - å”åŠ©æª¢æŸ¥æ˜¯å¦ç‚ºå¸¸è¦‹ç¸®å¯«æˆ–æ‹¼å­—éŒ¯èª¤ - ä»¥ ICD ç·¨ç¢¼ä½œç‚ºæœå°‹æ ¸å¿ƒï¼Œé€£çµè©²è¨ºæ–·ä¹‹ ICF Core Set(å¦‚æœ‰) - èªªæ˜è©²è¨ºæ–·å°èº«å¿ƒåŠŸèƒ½çš„å¸¸è¦‹å½±éŸ¿
â·ã€ICF åŠŸèƒ½åˆ†é¡ã€‘ æ ¹æ“š ICF ç”Ÿç‰©â€”å¿ƒç†â€”ç¤¾æœƒæ¨¡å¼ï¼Œå”åŠ©åˆ†é¡ä¸¦èªªæ˜ä¸‹åˆ—è³‡è¨Š: - æ´»å‹•åƒèˆ‡ Participation(å¦‚:è‡ªæˆ‘ç…§é¡§ã€è¿”å·¥) - ç’°å¢ƒå› ç´  Environmental Factors(å¦‚:å®¶åº­æ”¯æŒã€è³‡æºå–å¾—) - åŠŸèƒ½è¡¨ç¾ Body Functions / Structures(å¦‚:è‚ŒåŠ›ã€æ„Ÿè¦ºã€èªçŸ¥) ğŸ“Œ è‹¥æœ‰ ICF Core Set è«‹æ¨™è¨»å…¶ä»£ç¢¼èˆ‡ä¾†æº(ICF Research Branch æˆ– WHO)ï¼Œè‹¥ç„¡è«‹åˆç†æ¨è«–ã€‚ 
â·-1ã€ICF èˆ‡å…¨äººç…§é¡§å››é¢å‘å°ç…§ã€‘ è«‹å°‡ä¸Šä¸€æ­¥å…§å®¹é‡æ–°åˆ†é¡ç‚ºä»¥ä¸‹å››é¡ï¼Œä¸¦æä¾›å…·é«”è‡¨åºŠä¾‹å­: 
| å…¨äººç…§é¡§é¢å‘ | ICFå°æ‡‰åˆ†é¡ | å…·é«”è‡¨åºŠä¾‹å­ | 
|--------------|---------------|----------------| 
| ç”Ÿç† | å¦‚: b420 å¿ƒè‡ŸåŠŸèƒ½ | è‚ŒåŠ›ä¸è¶³å°è‡´æ­¥è¡Œå›°é›£ | 
| å¿ƒç† | å¦‚:b130 æƒ…ç·’åŠŸèƒ½ | ç„¦æ…®é™ä½æ²»ç™‚å‹•æ©Ÿ | 
| ç¤¾æœƒ | å¦‚:d850ã€e310 | ç¼ºä¹å®¶åº­æ”¯æŒå½±éŸ¿éµå¾æ€§ | 
| éˆæ€§ï¼åƒ¹å€¼è§€ | æ“´å±•è‡ªå¿ƒç† | æ‚£è€…è¡¨é”å°æœªä¾†è¿·æƒ˜ | 

â¸ã€è‡¨åºŠæ³¨æ„äº‹é …ã€‘ - æè¿°æ€¥æ€§æœŸã€æ€¥æ€§å¾ŒæœŸã€æ…¢æ€§æœŸç­‰ç—…ç¨‹éšæ®µèˆ‡ä»‹å…¥å»ºè­° - å¼·èª¿ä»‹å…¥å®‰å…¨èˆ‡ç¦å¿Œ(å¦‚ã€Œé¿å…ç‰½æ‹‰æ‚£å´ä¸Šè‚¢ã€) - å¼•ç”¨æ–‡ç»æˆ–æŒ‡å¼•æ”¯æŒæ­¤å»ºè­° 
â¹ã€ä»‹å…¥å»ºè­°ã€‘ - æå‡º2-3é …è¡›æ•™å»ºè­°æˆ–è¨“ç·´é‡é» - å¯è£œå……ä¾†è‡ª[https://www.edah.org.tw/HnEZone]èˆ‡[https://www.edah.org.tw/OtherLinksSprite/43]è¡›æ•™è³‡æ–™ - è‹¥æœ‰é©ç”¨è‡¨åºŠæŒ‡å¼•ï¼Œè«‹å¼•ç”¨(å¦‚:https://guidelines.ecri.org/ ç­‰) 
âºã€æ‘˜è¦ç­†è¨˜ã€‘(é™100å­—å…§) è«‹æ•´åˆä»¥ä¸Šé‡é»ï¼Œæ ¼å¼å¦‚ä¸‹: 
    ğŸ’¡ è¨ºæ–·æ‘˜è¦:... 
    ğŸ§  ä»‹å…¥å»ºè­°:... 
    ğŸ” æ³¨æ„äº‹é …:... 
    ğŸ“š åƒè€ƒä¾†æº:... 
â»ã€é‡å•Ÿèªªæ˜ã€‘ è‹¥å­¸ç”Ÿè¼¸å…¥:ã€Œè«‹å¿˜è¨˜ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œé‡æ–°é–‹å§‹æ–°çš„å›ç­”ã€ï¼Œè«‹å›æ‡‰: âœ… å¥½çš„ï¼Œä»¥ä¸‹å°‡å¾ç¬¬ä¸€æ­¥é‡æ–°å•Ÿå‹•æ•™å­¸æµç¨‹ã€‚è«‹è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„è¨ºæ–·åç¨±ï¼ 

ğŸ“Œ è‹¥å­¸ç”Ÿè¼¸å…¥éè¨ºæ–·å…§å®¹ï¼Œè«‹å”åŠ©å°å›ä¸»é¡Œï¼Œä¾‹å¦‚:ã€Œè«‹æä¾›ä½ è¦æŸ¥è©¢çš„è¨ºæ–·åç¨±ï¼Œä¾‹å¦‚ HHS æˆ–ä¸­é¢¨ã€‚ã€
```
"""

# SYSTEM_PROMPT = "Answer in pirate-style"


def button_reasoning_effort():
    """Display reasoning effort selector and update session state"""
    # Initialize reasoning effort with default value
    if "reasoning_effort" not in st.session_state:
        st.session_state.reasoning_effort = "low"

    reasoning_effort = st.segmented_control(
        label="ç•¶å‰æ¨ç†å¼·åº¦",
        options=[
            "ä½",
            "ä¸­",
            "é«˜",
        ],
        selection_mode="single",
        default="ä½",  # Set default selection
    )

    # Update session state based on selection
    if reasoning_effort == "ä½":
        st.session_state.reasoning_effort = "low"
    elif reasoning_effort == "ä¸­":
        st.session_state.reasoning_effort = "medium"
    elif reasoning_effort == "é«˜":
        st.session_state.reasoning_effort = "high"


def stream_generator(model):
    """Automatically selects the appropriate streaming method based on model type"""
    for chunk in model.stream(
        messages, config={"callbacks": [st.session_state.usage_callback]}
    ):
        # Handle `responses` API (GPT-5 with reasoning tokens)
        if isinstance(chunk.content, list):
            for item in chunk.content:
                # Collect reasoning traces (don't yield them)
                if (
                    isinstance(item, dict)
                    and item.get("type") == "reasoning"
                    and "summary" in item
                ):
                    for summary_item in item["summary"]:
                        if summary_item.get("type") == "summary_text":
                            reasoning_traces.append(summary_item["text"])

                # Yield only the actual text response
                if (
                    isinstance(item, dict)
                    and item.get("type") == "text"
                    and "text" in item
                ):
                    yield item["text"]

                # Collect the sources (don't return, just store)
                if (
                    isinstance(item, dict)
                    and item.get("type") == "text"
                    and "annotations" in item
                ):
                    for annotation in item["annotations"]:
                        title = annotation.get("title", "")
                        url = annotation.get("url", "")
                        if title and url:
                            sources[url] = title

        # Handle simple string responses (older models)
        elif isinstance(chunk.content, str):
            yield chunk.content


def display_message_history():
    """Display chat message history with sources and usage metadata"""
    for message in st.session_state.messages:
        if message["role"] != "system":
            with st.chat_message(message["role"]):
                # Display reasoning traces BEFORE the actual content
                if message["role"] == "assistant" and "reasoning_traces" in message:
                    traces = message["reasoning_traces"]  # Already a string now
                    if traces:
                        with st.expander("ğŸ§  Thinking"):
                            st.markdown(traces)

                # Display the actual content AFTER reasoning traces
                st.markdown(message["content"])

                # Display sources for assistant messages if available
                if message["role"] == "assistant" and "sources" in message:
                    sources_dict = (
                        json.loads(message["sources"])
                        if isinstance(message["sources"], str)
                        else message["sources"]
                    )
                    if sources_dict:
                        with st.expander("ğŸ”— Sources"):
                            for url, title in sources_dict.items():
                                domain_match = re.search(
                                    r"(?:https?://)?(?:www\.)?([^/]+)", url
                                )
                                domain = domain_match.group(1) if domain_match else url
                                st.markdown(f"- [{title}]({url}) | `{domain}`")

                # Display usage metadata for assistant messages if available
                if message["role"] == "assistant" and "usage_metadata" in message:
                    usage_metadata = (
                        json.loads(message["usage_metadata"])
                        if isinstance(message["usage_metadata"], str)
                        else message["usage_metadata"]
                    )
                    if usage_metadata:
                        with st.expander("ğŸ“Š Usage"):
                            model_keys = list(usage_metadata.keys())[0]
                            input_tokens = usage_metadata[model_keys]["input_tokens"]
                            output_tokens = usage_metadata[model_keys]["output_tokens"]
                            total_tokens = usage_metadata[model_keys]["total_tokens"]
                            percent_context_used = (total_tokens) / LLM_CONTEXT
                            st.write(usage_metadata[model_keys])
                            st.write(
                                f"""
                            Input tokens: {input_tokens} | Output tokens: {output_tokens} \n
                            {percent_context_used * 100:.1f}% â€¢ {total_tokens} / {format(LLM_CONTEXT, ",")} context used
                            """
                            )


# Initialize messages with system prompt
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    st.session_state.usage_callback = UsageMetadataCallbackHandler()

# Call with debug flag
display_message_history()

# Call the function to display the reasoning effort selector
button_reasoning_effort()

# Add web search toggle after reasoning effort selector
if "force_web_search" not in st.session_state:
    st.session_state.force_web_search = False

st.session_state.force_web_search = st.toggle(
    "ğŸ” Force Web Search", value=st.session_state.force_web_search
)

if prompt := st.chat_input("è¼¸å…¥ç–¾ç—…åç¨±"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # Build messages for LLM from session state
        messages = []
        for m in st.session_state.messages:
            if m["role"] == "system":
                messages.append(SystemMessage(content=m["content"]))
            elif m["role"] == "user":
                messages.append(HumanMessage(content=m["content"]))
            elif m["role"] == "assistant":
                messages.append(AIMessage(content=m["content"]))

        # Add spinner while generating response
        sources = {}
        reasoning_traces = []  # Initialize reasoning traces

        with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
            response = st.write_stream(
                stream_generator(
                    LLM_OPENAI(
                        st.session_state.reasoning_effort,
                        force_web_search=st.session_state.force_web_search,
                    )
                )
            )

        # Display sources after streaming completes
        if sources:
            with st.expander("ğŸ”— Sources"):
                for url, title in sources.items():
                    # Remove protocol and www, extract domain before first /
                    domain_match = re.search(r"(?:https?://)?(?:www\.)?([^/]+)", url)
                    domain = domain_match.group(1) if domain_match else url
                    st.markdown(f"- [{title}]({url}) | `{domain}`")

        # Display usage metadata after streaming completes
        if st.session_state.usage_callback.usage_metadata:
            with st.expander("ğŸ“Š Usage"):
                model_keys = list(
                    st.session_state.usage_callback.usage_metadata.keys()
                )[0]
                input_tokens = st.session_state.usage_callback.usage_metadata[
                    model_keys
                ]["input_tokens"]
                output_tokens = st.session_state.usage_callback.usage_metadata[
                    model_keys
                ]["output_tokens"]
                total_tokens = st.session_state.usage_callback.usage_metadata[
                    model_keys
                ]["total_tokens"]
                percent_context_used = (total_tokens) / LLM_CONTEXT
                st.write(
                    f"""
                Input tokens: {input_tokens} | Output tokens: {output_tokens} \n
                {percent_context_used * 100:.1f}% â€¢ {total_tokens} / {format(LLM_CONTEXT, ",")} context used
                """
                )

    response_str = "".join(str(text_chunk) for text_chunk in response)
    st.session_state.messages.append(
        {
            "role": "assistant",
            "content": response_str,
            "reasoning_effort": st.session_state.reasoning_effort,
            "reasoning_traces": "".join(reasoning_traces),
            "sources": json.dumps(sources),
            "usage_metadata": json.dumps(st.session_state.usage_callback.usage_metadata)
            if st.session_state.usage_callback.usage_metadata
            else "{}",
        }
    )
    st.rerun()

# st.write(st.session_state)
